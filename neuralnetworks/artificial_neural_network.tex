\documentclass[20pt]{article}
\begin{document}

\section{Problems of Linear/Polynomial Regression}

\begin{itemize}
    \item The linear regression function $y' = F(x)$ maps $x$ to an approximate value $y'$ that should be as close as possible to the ground truth value $y$.
    \item Although this model works for a lot of senarios, it still cannot handle \textbf{complex non-linear relationships}.
    \item One of the problems of polynomial transofrmation is that it generates a huge number of new features (if you have 10,000 features and you want to do a polynomial transformation of length 2, the result would be 50 million!).
    \item Also, linear and polynomial regression has a very limited space transformation shapes.
\end{itemize}

\subsection{Why linear and polynomial regression has limited transformations?}
\begin{itemize}
    \item The only shape this model can learn is through \textbf{powers of $x$}, that restricts the ability to learn from complex patterns that might be better modeled by other transformations (like sinusoids, exponentials, step functions, etc).
    \item Changing the coefficient of any term (e.g. $x^3$) affects \textbf{the entire curve globally}, not locally. This means that you can't fit small local variations easily, also overfitting or oscillations can happen very quickly with high-degree polynomials.
    \item \textbf{Global Influence:} Suppose you have the model $y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3$ and you decided to tweak $w_3$ (the coefficient of $x_3$) a little bit, the whole curve changes, not just around one point.
    \item Outside the range of data, polynomial functions grow quickly and unpredictably (especially higher-order ones). \textbf{So it lacks generalization power}.
\end{itemize}

Artificial Neural Networks (ANN) is a very powerful way to handle these limitations and perform non-linear transformations with a much richer transformation space.

\section{Activation Functions}
What is the point of activation functions?
\begin{itemize}
    \item \textbf{Important: } A neural network with linear activativation is just a normal linear regression model.
    \item The solution is not generating polynomial features, if we did this then we didn't solve the original problem of polynomial regression.
    \item The solution is \textbf{Adding a differentiable non-linear function to the output of each neuron}. Now we forced the neuron output to get into a new non-linear transformation.
    \item The neural network now is really mapping the input to completely different space.
    \item Popular activation functions: Sigmoid, tanh, and Relu.
    \item \textbf{Note: } In linear regression, there is no need for an activation function, because we want to keep the aggregated sum linear. So we use a dummy act. function called $identity(x) = x$
\end{itemize}

\end{document}
